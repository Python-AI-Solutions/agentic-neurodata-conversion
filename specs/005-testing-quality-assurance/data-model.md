# Testing and Quality Assurance Framework - Data Model

**Feature**: 005-testing-quality-assurance
**Date**: 2025-10-03
**Status**: Complete

This document defines the key entities, their fields, relationships, and validation rules for the testing and quality assurance framework.

## Entity Definitions

### 1. Test Suite

**Description**: A collection of related test cases organized by component, functionality, or test type (unit, integration, end-to-end).

**Fields**:
- `id` (str): Unique identifier (e.g., "mcp-server-suite", "agent-unit-suite")
- `name` (str): Human-readable name
- `type` (enum): Type of suite ("unit", "integration", "e2e", "validation", "performance")
- `test_cases` (List[TestCase]): Collection of test cases in this suite
- `setup_fixtures` (List[str]): Required pytest fixtures for suite
- `markers` (List[str]): pytest markers applied to suite ("slow", "requires_datasets", etc.)
- `execution_time_seconds` (float): Total execution time
- `parallel_safe` (bool): Can tests run in parallel
- `dependencies` (List[str]): Other suites that must run first

**Relationships**:
- Contains many `TestCase` entities
- References `TestFixture` entities for setup
- Belongs to one `CI Pipeline` entity

**Validation Rules**:
- `id` must be unique across all test suites
- `type` must be one of the defined enum values
- `execution_time_seconds` must be positive
- If `parallel_safe` is False, suite must run sequentially

---

### 2. Test Case

**Description**: An individual test that validates a specific behavior, requirement, or condition with defined inputs, execution steps, and expected outputs.

**Fields**:
- `id` (str): Unique identifier (e.g., "test_mcp_tool_endpoint_valid_request")
- `name` (str): Descriptive test name
- `functional_requirement` (str): Related FR (e.g., "FR-001")
- `test_function` (str): Python function name
- `file_path` (Path): Path to test file
- `markers` (List[str]): pytest markers ("asyncio", "slow", "flaky", etc.)
- `fixtures_required` (List[str]): Fixture names needed
- `assertions` (List[str]): What is being asserted
- `execution_time_ms` (float): Execution time in milliseconds
- `status` (enum): "passed", "failed", "skipped", "xfailed"
- `failure_message` (Optional[str]): Error message if failed
- `flaky_count` (int): Number of times marked flaky

**Relationships**:
- Belongs to one `Test Suite`
- Uses multiple `Test Fixture` entities
- Generates `Test Artifact` entities on failure
- Contributes to `Coverage Report`

**Validation Rules**:
- `functional_requirement` must reference valid FR from spec
- `execution_time_ms` must be positive
- `status` must be one of defined enum values
- If `flaky_count` > 3, test must be quarantined

---

### 3. Test Fixture

**Description**: Reusable test setup components including mock data, mock services, test environments, and helper functions used across multiple test cases.

**Fields**:
- `id` (str): Fixture name (e.g., "mock_knowledge_graph", "test_nwb_file")
- `scope` (enum): pytest scope ("function", "module", "session")
- `setup_code` (str): Python code for fixture setup
- `teardown_code` (str): Python code for cleanup
- `dependencies` (List[str]): Other fixtures required
- `data_path` (Optional[Path]): Path to fixture data files
- `is_mock` (bool): Whether fixture provides mocked dependency
- `setup_time_ms` (float): Time to set up fixture
- `shared_across_suites` (bool): Used by multiple test suites

**Relationships**:
- Used by many `Test Case` entities
- May depend on other `Test Fixture` entities
- May provide `Mock Service` instances

**Validation Rules**:
- `scope` must be valid pytest scope
- If `scope` is "session", fixture must be thread-safe
- `setup_time_ms` should be minimized for "function" scope
- Dependencies must not create circular references

---

### 4. Coverage Report

**Description**: A detailed report showing which code paths, functions, and branches have been exercised by tests, with metrics for overall coverage percentages.

**Fields**:
- `id` (str): Report identifier (timestamp-based)
- `timestamp` (datetime): When report was generated
- `total_coverage_percent` (float): Overall coverage percentage
- `line_coverage_percent` (float): Line coverage percentage
- `branch_coverage_percent` (float): Branch coverage percentage
- `files_covered` (Dict[str, float]): Per-file coverage percentages
- `uncovered_lines` (Dict[str, List[int]]): Lines not covered per file
- `format` (enum): "html", "xml", "json", "terminal"
- `output_path` (Path): Path to report file
- `meets_threshold` (bool): Coverage >= 80%

**Relationships**:
- Generated by `CI Pipeline`
- Aggregates coverage from multiple `Test Suite` entities
- Enforced by `Quality Gate`

**Validation Rules**:
- All coverage percentages must be between 0 and 100
- `total_coverage_percent` >= 80% for quality gate to pass
- `timestamp` must be in ISO 8601 format
- Output files must exist at `output_path`

---

### 5. Quality Gate

**Description**: A set of automated checks and thresholds that must pass before code changes can be merged, including test passage, coverage requirements, and code quality metrics.

**Fields**:
- `id` (str): Gate identifier (e.g., "pr-merge-gate", "release-gate")
- `name` (str): Human-readable name
- `checks` (List[Dict]): List of checks with thresholds
  - `check_type`: "test_pass_rate", "coverage", "linting", "type_checking"
  - `threshold`: Required value
  - `operator`: ">=", ">", "==", etc.
- `blocking` (bool): Whether gate blocks merge
- `applies_to` (List[str]): Branches gate applies to
- `status` (enum): "passing", "failing", "skipped"
- `failed_checks` (List[str]): Which checks failed

**Relationships**:
- Enforces `Coverage Report` thresholds
- Requires all `Test Suite` entities to pass
- Part of `CI Pipeline`

**Validation Rules**:
- At least one check must be defined
- Thresholds must be valid for check type
- If `blocking` is True, must be in CI pipeline
- `applies_to` must reference valid branches

**Example Configuration**:
```yaml
quality_gate:
  id: "pr-merge-gate"
  checks:
    - check_type: "test_pass_rate"
      threshold: 100
      operator: "=="
    - check_type: "coverage"
      threshold: 80
      operator: ">="
    - check_type: "linting"
      threshold: 0
      operator: "=="  # Zero linting errors
```

---

### 6. Mock Service

**Description**: A simulated version of an external dependency (Knowledge Graph, DataLad, file system) used in tests to provide controlled, predictable behavior without real external calls.

**Fields**:
- `id` (str): Mock identifier (e.g., "mock_kg", "mock_datalad")
- `service_type` (enum): "knowledge_graph", "datalad", "filesystem", "http_api"
- `implementation_class` (str): Python class implementing mock
- `default_behavior` (Dict): Default responses/behaviors
- `configurable_responses` (bool): Can responses be parameterized
- `records_calls` (bool): Records calls for verification
- `call_history` (List[Dict]): History of calls made to mock
- `failure_modes` (List[str]): Simulated failure scenarios

**Relationships**:
- Provided by `Test Fixture` entities
- Used by `Test Case` entities
- Part of testing infrastructure

**Validation Rules**:
- `service_type` must match a real external dependency
- If `records_calls` is True, `call_history` must be clearable
- `default_behavior` must match service API contract
- Failure modes must simulate realistic error conditions

**Example Mock Configuration**:
```python
mock_knowledge_graph = MockService(
    id="mock_kg",
    service_type="knowledge_graph",
    default_behavior={
        "query_response": {"results": []},
        "insert_response": {"success": True}
    },
    records_calls=True,
    failure_modes=["network_timeout", "invalid_sparql", "connection_refused"]
)
```

---

### 7. Test Artifact

**Description**: Outputs generated during test execution including logs, screenshots, data dumps, performance profiles, and failure diagnostics preserved for debugging.

**Fields**:
- `id` (str): Artifact identifier
- `test_case_id` (str): Associated test case
- `artifact_type` (enum): "log", "screenshot", "dump", "profile", "trace"
- `file_path` (Path): Path to artifact file
- `size_bytes` (int): Artifact size
- `created_at` (datetime): When artifact was created
- `retention_days` (int): How long to keep artifact
- `ci_run_id` (Optional[str]): CI run that generated artifact

**Relationships**:
- Generated by `Test Case` on failure or request
- Stored by `CI Pipeline`
- Cleaned up based on retention policy

**Validation Rules**:
- `artifact_type` must be valid enum value
- `file_path` must point to existing file
- `size_bytes` must match actual file size
- Artifacts older than `retention_days` should be deleted

---

### 8. Validation Rule

**Description**: A specific check applied to test outputs (especially NWB files) that verifies conformance to schemas, business rules, or quality standards.

**Fields**:
- `id` (str): Rule identifier (e.g., "nwb_schema_conformance")
- `name` (str): Human-readable rule name
- `category` (enum): "schema", "best_practice", "custom", "dandi_compliance"
- `validator` (str): Python function/class implementing validation
- `severity` (enum): "error", "warning", "info"
- `required` (bool): Must pass for validation to succeed
- `parameters` (Dict): Configurable parameters
- `error_message_template` (str): Template for error messages

**Relationships**:
- Applied during `Test Case` execution
- Groups into validation layers (schema → best practices → custom)
- Results included in `Test Artifact` reports

**Validation Rules**:
- `severity` must be "error", "warning", or "info"
- If `required` is True, `severity` must be "error"
- `validator` must be importable Python callable
- Parameters must match validator signature

**Example Rules**:
```yaml
validation_rules:
  - id: "nwb_schema_v2_6"
    name: "NWB Schema 2.6 Conformance"
    category: "schema"
    validator: "pynwb.validate"
    severity: "error"
    required: true

  - id: "ephys_sample_rate"
    name: "Ephys Minimum Sample Rate"
    category: "custom"
    validator: "validators.check_ephys_sample_rate"
    severity: "warning"
    required: false
    parameters:
      min_sample_rate: 20000
```

---

### 9. CI Pipeline

**Description**: The automated workflow that executes on code commits, running tests, quality checks, and reporting results back to developers.

**Fields**:
- `id` (str): Pipeline identifier
- `name` (str): Pipeline name (e.g., "Unit Tests", "Integration Tests")
- `trigger` (enum): "push", "pull_request", "schedule", "manual"
- `branches` (List[str]): Branches that trigger pipeline
- `test_suites` (List[str]): Test suite IDs to run
- `quality_gates` (List[str]): Quality gate IDs to enforce
- `parallel_jobs` (int): Number of parallel jobs
- `timeout_minutes` (int): Maximum pipeline duration
- `artifacts_to_upload` (List[str]): Artifact types to preserve
- `status` (enum): "pending", "running", "success", "failure", "cancelled"

**Relationships**:
- Executes multiple `Test Suite` entities
- Enforces `Quality Gate` entities
- Generates `Coverage Report` and `Test Artifact` entities

**Validation Rules**:
- `trigger` must be valid GitHub Actions trigger
- `timeout_minutes` must be positive
- `parallel_jobs` must not exceed available runners
- At least one test suite must be specified

**Example Pipeline Configuration**:
```yaml
ci_pipeline:
  id: "unit-tests"
  name: "Unit Tests"
  trigger: "push"
  branches: ["main", "develop", "feature/*"]
  test_suites: ["mcp-server-suite", "agent-unit-suite"]
  quality_gates: ["pr-merge-gate"]
  parallel_jobs: 4
  timeout_minutes: 10
```

---

### 10. Flaky Test

**Description**: A test that exhibits non-deterministic behavior, sometimes passing and sometimes failing for the same code, typically due to timing issues, race conditions, or environmental factors.

**Fields**:
- `test_case_id` (str): ID of flaky test case
- `detection_date` (datetime): When flakiness was first detected
- `pass_rate_percent` (float): Percentage of runs that pass
- `total_runs` (int): Total number of runs tracked
- `failure_patterns` (List[str]): Common failure modes
- `suspected_causes` (List[str]): Hypothesized root causes
- `status` (enum): "investigating", "quarantined", "fixed", "removed"
- `assigned_to` (Optional[str]): Developer investigating
- `fix_deadline` (Optional[datetime]): Target fix date

**Relationships**:
- References a `Test Case` entity
- May generate multiple `Test Artifact` entities for debugging
- Tracked in `Test Metrics Dashboard`

**Validation Rules**:
- `pass_rate_percent` must be > 0 and < 100 (0% or 100% = not flaky)
- `total_runs` must be >= 5 for statistical significance
- If `status` is "quarantined", test must have `@pytest.mark.flaky`
- If `fix_deadline` is past, escalate to team lead

---

### 11. Performance Benchmark

**Description**: A test that measures execution time, memory usage, or other performance metrics and compares them against baseline thresholds to detect regressions.

**Fields**:
- `id` (str): Benchmark identifier
- `test_case_id` (str): Associated test case
- `metric_name` (str): What is being measured (e.g., "conversion_time_ms")
- `metric_value` (float): Current measured value
- `metric_unit` (str): Unit of measurement
- `baseline_value` (float): Expected/historical value
- `threshold_percent` (float): Allowed deviation from baseline
- `exceeded_threshold` (bool): Whether current run exceeded threshold
- `timestamp` (datetime): When benchmark was run
- `environment` (Dict): System info (CPU, memory, etc.)

**Relationships**:
- Associated with one `Test Case`
- Tracked over time in `Test Metrics Dashboard`
- May block merge via `Quality Gate` if threshold exceeded

**Validation Rules**:
- `metric_value` and `baseline_value` must be positive
- `threshold_percent` typically 10-20% for stability
- `exceeded_threshold` = (|metric_value - baseline_value| / baseline_value) > threshold_percent
- Environment info should include Python version, OS, CPU

**Example Benchmark**:
```python
benchmark = PerformanceBenchmark(
    id="nwb_conversion_benchmark",
    test_case_id="test_full_conversion_workflow",
    metric_name="conversion_time_seconds",
    metric_value=28.5,
    baseline_value=25.0,
    threshold_percent=20,  # 20% degradation allowed
    exceeded_threshold=False  # 28.5 is within 20% of 25
)
```

---

### 12. Integration Test Environment

**Description**: A dedicated testing environment configured to mirror production settings where integration and end-to-end tests execute against live service instances.

**Fields**:
- `id` (str): Environment identifier (e.g., "integration", "staging")
- `name` (str): Human-readable name
- `services` (List[Dict]): Deployed services
  - `service_name`: Name of service
  - `version`: Service version
  - `endpoint`: Service URL/connection string
- `datasets` (List[str]): Available test datasets
- `configuration` (Dict): Environment variables, configs
- `status` (enum): "available", "busy", "maintenance", "failed"
- `created_at` (datetime): When environment was provisioned
- `auto_cleanup` (bool): Automatically clean up after tests

**Relationships**:
- Used by `Test Suite` entities of type "integration" or "e2e"
- Managed by `CI Pipeline`
- Provides data for `Test Case` execution

**Validation Rules**:
- All services must be reachable at their endpoints
- Datasets must be accessible and valid
- If `auto_cleanup` is True, cleanup must run after tests
- Environment should mirror production configuration

---

### 13. Test Metrics Dashboard

**Description**: A visualization interface displaying test execution statistics, coverage trends, failure rates, and quality indicators over time.

**Fields**:
- `id` (str): Dashboard identifier
- `metrics` (Dict[str, List]): Time-series metrics
  - `test_pass_rate`: Pass rate over time
  - `coverage_trend`: Coverage percentage over time
  - `execution_time_trend`: Total test time over time
  - `flaky_test_count`: Number of flaky tests over time
- `time_range` (str): Data time range (e.g., "30d", "90d")
- `refresh_interval_minutes` (int): How often to update
- `alerts` (List[Dict]): Active alerts
  - `metric`: Metric that triggered alert
  - `condition`: Alert condition
  - `triggered_at`: When alert fired
- `visualizations` (List[str]): Chart types ("line", "bar", "heatmap")

**Relationships**:
- Aggregates data from `Test Suite` and `Coverage Report` entities
- Displays `Flaky Test` and `Performance Benchmark` trends
- Consumed by developers and CI systems

**Validation Rules**:
- `time_range` must be valid duration format
- `refresh_interval_minutes` must be positive
- Metrics must have at least 2 data points for trends
- Alerts must reference valid metrics and conditions

**Example Metrics**:
```yaml
dashboard_metrics:
  test_pass_rate:
    - timestamp: "2025-10-01T10:00:00Z"
      value: 98.5
    - timestamp: "2025-10-02T10:00:00Z"
      value: 99.2
  coverage_trend:
    - timestamp: "2025-10-01T10:00:00Z"
      value: 82.3
    - timestamp: "2025-10-02T10:00:00Z"
      value: 83.1
```

---

## Entity Relationships Diagram

```
CI Pipeline
    ├── executes → Test Suite (1:N)
    │   └── contains → Test Case (1:N)
    │       ├── uses → Test Fixture (N:M)
    │       │   └── provides → Mock Service (1:1 or 1:N)
    │       ├── generates → Test Artifact (1:N)
    │       ├── validates with → Validation Rule (N:M)
    │       └── may be → Flaky Test (1:1)
    ├── enforces → Quality Gate (1:N)
    │   └── checks → Coverage Report (1:1)
    └── uploads to → Test Metrics Dashboard (N:1)

Performance Benchmark
    └── associated with → Test Case (N:1)

Integration Test Environment
    └── used by → Test Suite (N:M)
```

---

## State Transitions

### Test Case State Machine
```
pending → running → passed
                 ├→ failed → [generates artifacts]
                 ├→ skipped
                 └→ xfailed (expected failure)
```

### CI Pipeline State Machine
```
pending → running → success
                 ├→ failure → [can retry]
                 └→ cancelled
```

### Flaky Test State Machine
```
detected → investigating → quarantined → fixed → removed
                                       └→ removed (unfixable)
```

---

## Data Validation Summary

### Coverage Thresholds
- Total coverage: ≥ 80%
- MCP endpoints: ≥ 90%
- Client libraries: ≥ 85%
- Critical paths: ≥ 95%

### Performance Thresholds
- Unit test total: < 5 minutes
- Integration test total: < 15 minutes
- E2E test total: < 30 minutes
- Individual test: < 30 seconds (warning if exceeded)

### Quality Gates
- All tests must pass (100% pass rate)
- Coverage thresholds met
- Zero critical linting errors
- Type checking passes
- No new flaky tests introduced

This data model provides the foundation for implementing the 46 functional requirements and tracking all aspects of the testing and quality assurance framework.
