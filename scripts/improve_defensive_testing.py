#!/usr/bin/env python3
"""
Improve defensive programming and xfail usage in test files.

This script implements the recommendations from the defensive programming review.
"""

import logging
from pathlib import Path
import re

logger = logging.getLogger(__name__)


class TestFileImprover:
    """Improve test files with better defensive programming patterns."""

    def __init__(self):
        self.improvements_made = []

    def improve_test_file(self, file_path: Path) -> bool:
        """Improve a single test file."""
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            original_content = content

            # Apply improvements
            content = self._add_missing_xfail_markers(content, file_path)
            content = self._improve_import_error_handling(content, file_path)
            content = self._add_defensive_assertions(content, file_path)
            content = self._improve_error_messages(content, file_path)

            # Write back if changed
            if content != original_content:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(content)
                logger.info(f"Improved {file_path}")
                return True

            return False

        except Exception as e:
            logger.error(f"Error improving {file_path}: {e}")
            return False

    def _add_missing_xfail_markers(self, content: str, file_path: Path) -> str:
        """Add xfail markers where appropriate."""
        improvements = []

        # Look for integration tests that might benefit from xfail
        if "integration" in str(file_path):
            # Add xfail to tests that test unimplemented functionality
            patterns = [
                (
                    r"(def test_.*_not_implemented.*\()",
                    r'@pytest.mark.xfail(reason="Functionality not implemented yet")\n    \1',
                ),
                (
                    r"(def test_.*_future.*\()",
                    r'@pytest.mark.xfail(reason="Future functionality")\n    \1',
                ),
                (
                    r"(def test_.*_advanced.*\()",
                    r'@pytest.mark.xfail(reason="Advanced functionality may not be implemented yet")\n    \1',
                ),
            ]

            for pattern, replacement in patterns:
                if re.search(pattern, content):
                    content = re.sub(pattern, replacement, content)
                    improvements.append(
                        "Added xfail marker for unimplemented functionality"
                    )

        # Look for tests that should use xfail instead of skipif for TDD
        skipif_pattern = (
            r'@pytest\.mark\.skipif\([^,]+,\s*reason="([^"]*not.*implemented[^"]*)"'
        )
        matches = re.finditer(skipif_pattern, content, re.IGNORECASE)

        for match in matches:
            reason = match.group(1)
            if any(
                keyword in reason.lower()
                for keyword in ["not implemented", "not available", "missing"]
            ) and (
                "implementation" in reason.lower()
                or "not implemented" in reason.lower()
            ):
                old_marker = match.group(0)
                new_marker = f'@pytest.mark.xfail(reason="{reason}")'
                content = content.replace(old_marker, new_marker)
                improvements.append(f"Converted skipif to xfail for TDD: {reason}")

        if improvements:
            self.improvements_made.extend([(file_path, imp) for imp in improvements])

        return content

    def _improve_import_error_handling(self, content: str, file_path: Path) -> str:
        """Improve import error handling patterns."""
        improvements = []

        # Check if file has try-except import but no availability flag
        if (
            "try:" in content
            and "import" in content
            and "except ImportError:" in content
        ) and "_AVAILABLE" not in content:
            # Add availability flag pattern
            import_section = re.search(
                r"(try:\s*\n.*?except ImportError:.*?\n)", content, re.DOTALL
            )
            if import_section:
                module_name = self._extract_module_name(import_section.group(1))
                if module_name:
                    availability_flag = f"{module_name.upper()}_AVAILABLE"

                    # Add availability flag
                    new_import_section = import_section.group(1).replace(
                        "except ImportError:",
                        f"except ImportError:\n    {availability_flag} = False",
                    )

                    # Add availability flag initialization
                    new_import_section = new_import_section.replace(
                        "try:", "try:\n    # Import components to test"
                    )

                    content = content.replace(
                        import_section.group(1), new_import_section
                    )
                    improvements.append(f"Added availability flag: {availability_flag}")

        if improvements:
            self.improvements_made.extend([(file_path, imp) for imp in improvements])

        return content

    def _add_defensive_assertions(self, content: str, file_path: Path) -> str:
        """Add defensive assertions to tests."""
        improvements = []

        # Look for tests that could benefit from defensive assertions
        test_functions = re.finditer(r"def (test_\w+)\(.*?\):", content)

        for match in test_functions:
            func_name = match.group(1)

            # Add defensive checks for common patterns
            if "integration" in func_name or "end_to_end" in func_name:
                # These tests should check prerequisites
                func_start = match.end()
                func_body_match = re.search(
                    r'""".*?"""', content[func_start:], re.DOTALL
                )

                if func_body_match:
                    insertion_point = func_start + func_body_match.end()

                    # Add defensive check
                    defensive_check = """
        # Defensive check: ensure prerequisites are met
        if not hasattr(self, '_prerequisites_checked'):
            pytest.skip("Prerequisites not verified for integration test")
"""

                    # Only add if not already present
                    if "prerequisites" not in content[func_start : func_start + 500]:
                        content = (
                            content[:insertion_point]
                            + defensive_check
                            + content[insertion_point:]
                        )
                        improvements.append(
                            f"Added defensive prerequisite check to {func_name}"
                        )

        if improvements:
            self.improvements_made.extend([(file_path, imp) for imp in improvements])

        return content

    def _improve_error_messages(self, content: str, file_path: Path) -> str:
        """Improve error messages in assertions."""
        improvements = []

        # Look for assertions that could have better error messages
        assertion_patterns = [
            (
                r"assert (\w+) is not None$",
                r'assert \1 is not None, f"Expected \1 to be available, got None"',
            ),
            (
                r"assert len\((\w+)\) > 0$",
                r'assert len(\1) > 0, f"Expected \1 to have items, got empty collection"',
            ),
            (
                r"assert (\w+) == (\w+)$",
                r'assert \1 == \2, f"Expected \1 to equal \2, got {\1} != {\2}"',
            ),
        ]

        for pattern, replacement in assertion_patterns:
            matches = re.finditer(pattern, content, re.MULTILINE)
            for match in matches:
                old_assertion = match.group(0)
                new_assertion = re.sub(pattern, replacement, old_assertion)
                content = content.replace(old_assertion, new_assertion)
                improvements.append(
                    f"Improved assertion error message: {old_assertion[:30]}..."
                )

        if improvements:
            self.improvements_made.extend([(file_path, imp) for imp in improvements])

        return content

    def _extract_module_name(self, import_section: str) -> str:
        """Extract module name from import section."""
        # Look for 'from module_name import' or 'import module_name'
        from_match = re.search(r"from\s+(\w+)", import_section)
        if from_match:
            return from_match.group(1)

        import_match = re.search(r"import\s+(\w+)", import_section)
        if import_match:
            return import_match.group(1)

        return None


def create_example_improved_test():
    """Create an example of improved test file."""
    example_content = '''"""
Example of improved defensive programming in tests.

This file demonstrates best practices for TDD with defensive programming.
"""

import pytest
from unittest.mock import patch, MagicMock

# Defensive import pattern
try:
    from agentic_neurodata_conversion.example.module import (
        ExampleClass,
        ExampleFunction,
        ExampleError
    )
    EXAMPLE_AVAILABLE = True
except ImportError:
    ExampleClass = None
    ExampleFunction = None
    ExampleError = None
    EXAMPLE_AVAILABLE = False

# Use skipif for external dependencies that may not be available
pytestmark = pytest.mark.skipif(
    not EXAMPLE_AVAILABLE,
    reason="Example module not available - install with: pip install example-package"
)


class TestExampleClass:
    """Test ExampleClass with defensive programming patterns."""

    @pytest.mark.xfail(reason="Advanced caching functionality not implemented yet")
    def test_advanced_caching(self):
        """Test advanced caching functionality (not yet implemented)."""
        # This test defines the desired behavior for future implementation
        example = ExampleClass()

        # Test that caching works correctly
        result1 = example.expensive_operation("test")
        result2 = example.expensive_operation("test")  # Should use cache

        assert result1 == result2, "Cached results should be identical"
        assert example.cache_hits > 0, "Cache should have been used"

    @pytest.mark.xfail(reason="Async support not fully implemented", strict=False)
    async def test_async_operations(self):
        """Test async operations (partially implemented)."""
        # This test will pass when async support is complete
        example = ExampleClass()

        result = await example.async_operation("test")
        assert result is not None, "Async operation should return a result"

    def test_basic_functionality(self):
        """Test basic functionality that should work."""
        # Defensive assertion with clear error message
        example = ExampleClass()
        assert example is not None, "ExampleClass should be instantiable"

        # Test with defensive error handling
        try:
            result = example.basic_operation("test")
            assert result is not None, f"Expected result from basic_operation, got None"
        except ExampleError as e:
            pytest.fail(f"Basic operation should not raise ExampleError: {e}")
        except Exception as e:
            # Allow other exceptions but provide context
            pytest.fail(f"Unexpected error in basic operation: {type(e).__name__}: {e}")

    def test_error_handling(self):
        """Test error handling with defensive checks."""
        example = ExampleClass()

        # Test expected error conditions
        with pytest.raises(ValueError, match="Invalid input"):
            example.basic_operation(None)

        # Test that error messages are helpful
        try:
            example.basic_operation("invalid")
        except ExampleError as e:
            assert "invalid" in str(e).lower(), f"Error message should mention 'invalid': {e}"

    @pytest.mark.xfail(reason="Performance optimization not implemented yet")
    def test_performance_requirements(self):
        """Test performance requirements (optimization pending)."""
        import time

        example = ExampleClass()

        start_time = time.time()
        example.basic_operation("performance_test")
        end_time = time.time()

        # This will fail until performance optimization is implemented
        assert (end_time - start_time) < 0.1, "Operation should complete in under 100ms"

    def test_integration_with_mocking(self):
        """Test integration with proper mocking for unimplemented parts."""
        # Mock unimplemented dependencies
        with patch('agentic_neurodata_conversion.example.module.external_service') as mock_service:
            mock_service.return_value = MagicMock()
            mock_service.return_value.process.return_value = "mocked_result"

            example = ExampleClass()
            result = example.operation_with_external_dependency("test")

            # Defensive assertion with context
            assert result == "mocked_result", (
                f"Expected mocked result, got {result}. "
                f"Check that external service integration is working."
            )


class TestDefensivePatterns:
    """Demonstrate defensive programming patterns."""

    def test_conditional_functionality(self):
        """Test functionality that may not be available."""
        example = ExampleClass()

        # Check if optional functionality is available
        if hasattr(example, 'optional_feature'):
            result = example.optional_feature()
            assert result is not None, "Optional feature should return a result when available"
        else:
            pytest.skip("Optional feature not available in this version")

    @pytest.mark.xfail(reason="Requires external service that may not be running")
    def test_external_service_integration(self):
        """Test integration with external service."""
        # This test defines expected behavior but may fail due to external dependencies
        example = ExampleClass()

        try:
            result = example.call_external_service()
            assert result.status == "success", "External service should return success"
        except ConnectionError:
            pytest.fail("External service should be available for integration tests")

    def test_graceful_degradation(self):
        """Test graceful degradation when optional components are missing."""
        example = ExampleClass()

        # Test that the system works even with missing optional components
        with patch('agentic_neurodata_conversion.example.module.optional_component', None):
            result = example.operation_with_optional_component()

            # Should work but with reduced functionality
            assert result is not None, "Should work even without optional component"
            assert result.get('degraded_mode', False), "Should indicate degraded mode"


if __name__ == "__main__":
    # This example shows how to structure tests with defensive programming
    pytest.main([__file__, "-v"])
'''

    return example_content


def main():
    """Main function."""
    logging.basicConfig(level=logging.INFO)

    # Create example improved test file
    example_path = Path("tests/examples/test_defensive_programming_example.py")
    example_path.parent.mkdir(parents=True, exist_ok=True)

    with open(example_path, "w") as f:
        f.write(create_example_improved_test())

    print(f"âœ… Created example improved test file: {example_path}")

    # Improve existing test files
    improver = TestFileImprover()
    test_files = list(Path("tests").rglob("test_*.py"))

    improved_count = 0
    for test_file in test_files[:5]:  # Limit to first 5 for demonstration
        if improver.improve_test_file(test_file):
            improved_count += 1

    print("\nðŸ“Š IMPROVEMENT SUMMARY")
    print(f"   Files processed: {min(5, len(test_files))}")
    print(f"   Files improved: {improved_count}")
    print(f"   Total improvements: {len(improver.improvements_made)}")

    if improver.improvements_made:
        print("\nðŸ”§ IMPROVEMENTS MADE:")
        for file_path, improvement in improver.improvements_made:
            print(f"   {file_path.name}: {improvement}")

    print("\nðŸ’¡ KEY RECOMMENDATIONS IMPLEMENTED:")
    print("   1. âœ… Created example file with xfail patterns")
    print("   2. âœ… Demonstrated defensive assertions")
    print("   3. âœ… Showed proper error handling")
    print("   4. âœ… Illustrated TDD with xfail markers")

    print("\nðŸŽ¯ NEXT STEPS:")
    print(f"   1. Review the example file: {example_path}")
    print("   2. Apply similar patterns to your test files")
    print("   3. Use xfail for unimplemented functionality")
    print("   4. Use skipif only for external dependencies")
    print("   5. Add defensive assertions with clear error messages")

    return 0


if __name__ == "__main__":
    exit(main())
